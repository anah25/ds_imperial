{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1dbc9899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from scipy.stats import bernoulli\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.special import psi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6555a4c",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation \n",
    "### Optimization Point of View"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8599082",
   "metadata": {},
   "source": [
    "### Formalizing MLE of Bernuolli Distributions\n",
    "Recall that in our previous MLE application, we were trying to estimate the probability of \"H\" showing after a random coin flip of a specific loaded coin. We did not focus on the details of how to derive such MLE estimation (where we were simply averaging the number of times where \"H\" shows up). Let us investigate this in more detail here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27c2b95",
   "metadata": {},
   "source": [
    "**Problem Source:** We borrow and modify the problem from Section 24.1 of ```Shalev-Shwartz, S. & Ben-David, S. (2014). Understanding machine learning: From theory to algorithms. Cambridge university press.```\n",
    "\n",
    "**Problem Description:** A drug company discovered a new drug that is hoped to treat COVID-19 in high-risk groups. You have a random sample of $n$ high-risk volunteering people and give each of them the new drug. Such training set can be represented as $\\mathcal{X} = (x_1, \\ldots, x_n)$ where $x_i =1$ if the $i$-th patient is recovered and $x_i = 0$ otherwise. We assume the true probability of survival is $\\theta$ for a single individual, i.e., the survival has a Bernuolli distribution. \n",
    "\n",
    "**Question:** How would you estimate $\\theta$ from $\\mathcal{X}$? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c644301",
   "metadata": {},
   "source": [
    "**Answer:** Previously we used the following estimation that is *very* intuitive\n",
    "$$\\hat{\\theta} = \\frac{1}{n} \\sum_{i=1}^n x_i.$$\n",
    "But what is special about this estimation? The first advantage is that it is unbiased. To see this, recall that when we do not see $\\mathcal{X}$ yet, we know that it is a collection of $n$ Bernuolli variables $X_i, \\ i=1,\\ldots,n$. We can derive:\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\mathcal{X}} [\\hat{\\theta}] & = \\frac{1}{n}\\mathbb{E}\\left[ {\\sum_{i=1}^n X_i} \\right] \\\\\n",
    "& = \\frac{1}{n} \\left( \\sum_{i=1}^n \\mathbb{E}\\left[ X_i \\right] \\right) \\\\\n",
    "& = \\frac{1}{n} \\left( \\sum_{i=1}^n  1 \\cdot \\mathbb{P}[X_i = 1] + 0 \\cdot \\mathbb{P}[X_i = 0] \\right) \\\\\n",
    "& = \\frac{1}{n} \\left( \\sum_{i=1}^n  \\theta \\right) \\\\\n",
    "& = \\theta.\n",
    "\\end{align*}\n",
    "So we can see that, for a randomly drawn sample of $n$ people, we expect our estimator $\\hat{\\theta}$ to be equal to $\\theta$. This is the property telling we have an *unbiased estimator of $\\theta$*. Let us summarize this process in a case study where $n=100$ and $\\theta = 0.8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ca50aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The true value of theta is 0.8 and our unbiased estimation is 0.85\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "theta = 0.8 #probability of survival\n",
    "n = 100 #number of patients in the trial\n",
    "sample = bernoulli.rvs(theta, size = 100) #take a single sample of 100-patients\n",
    "estimator = np.mean(sample)\n",
    "print(r\"The true value of theta is\", theta, \"and our unbiased estimation is\", estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0279f33",
   "metadata": {},
   "source": [
    "As we observe, there is a difference between $\\hat{\\theta}$ and $\\theta$, so we have some bias in our estimation. Then why do we claim $\\hat{\\theta}$ is unbiased? The answer is, our $\\hat{\\theta}$ only sees a single sample of $100$ patients, and there is no guarantee that these $100$ patients will have precisely $80$ survival (otherwise we are living in a world with no uncertainty). What unbiased estimator means is that, if we have the above experiment a lot of times, then the average of our estimations will be $0.8$. Let us observe this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9406279b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of all estimations is 0.799\n"
     ]
    }
   ],
   "source": [
    "simulations = 10000 #large number to iterate above experiment. We can think this as 10000 different worlds.\n",
    "theta = 0.8 #probability of survival\n",
    "n = 100 #number of patients in the trial\n",
    "estimators = np.zeros(simulations)\n",
    "for sim in range(simulations):\n",
    "    sample_temp = bernoulli.rvs(theta, size = 100) #take a new sample of 100-patients\n",
    "    estimators[sim] = np.mean(sample_temp)\n",
    "average_estimator = np.mean(estimators)\n",
    "print(\"The average of all estimations is\", np.round(average_estimator,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cbb173",
   "metadata": {},
   "source": [
    "**Discussion:** Now we understand what is an *unbiased estimator* of the true parameter of the underlying distribiution. In most cases, the *unbiased estimator* is not necessarily the *maximum likelihood estimator* (if, for example, $X_i$ were normally distributed, the maximum likelihood standard deviation estimator would not be unbiased). So it is important to understand the key difference between having an *unbiased estimator* and a *maximum likelihood estimator*. Let us first define the maximum likelihood estimation principle, which is very interesting both practically and philosophically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc370fb",
   "metadata": {},
   "source": [
    "**<span style=\"color:blue\">MLE STEP 1: Derive the likelihood function. </span>**\n",
    "We know that there is a true $\\theta$ from which each of the $X_i$ in our $\\mathcal{X}$ is sampled, and the realizations are $X_i = x_i$ for all $i=1,\\ldots,n$. Instead of asking \"does averaging $x_i$ work\", here we ask \"which $\\theta \\in [0,1]$ would be the *most likely* to generate such a sample $\\mathcal{X}$?\". For this purpose, we first write the probability of seeing such $X_i = x_i$ sequence for $i = 1,\\ldots,n$ for an arbitrary $\\theta$.\n",
    "\\begin{align*}\n",
    "\\mathbb{P}[X_1 =x_1, \\ldots, X_n = x_n] &= \\prod_{i=1}^n \\mathbb{P}[X_i =x_i] \\\\\n",
    "& =\\prod_{i=1}^n  \\{\\theta \\text{ if $x_i = 1$; otherwise } 1-\\theta \\} \\\\\n",
    "& =\\prod_{i=1}^n  \\theta^{x_i} (1- \\theta)^{1 - x_i} \\\\\n",
    "& = \\theta^{\\sum_{i=1}^n x_i} (1-\\theta)^{\\sum_{i=1}^n (1-x_i)}.\n",
    "\\end{align*}\n",
    "The *maximum likelihood estimator* $\\theta^\\star$ would be the $\\theta$ that maximizes this probability. However, maximizing a function $\\theta^{85} \\cdot (1-\\theta)^{15}$ is quite hard for most computers and solvers. How can we overcome this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6478255e",
   "metadata": {},
   "source": [
    "**<span style=\"color:blue\">MLE STEP 2: Derive the log-likelihood function. </span>**\n",
    "We take $\\ell(\\theta) := \\log \\mathbb{P}[X_1 =x_1, \\ldots, X_n = x_n]$ as the log-likelihood function. The trick is, logarithm is a non-decreasing function. Hence, the $\\theta^\\star$ that maximizes the likelihood function will also maximize the log-likelihood function. In this case, we apply simple logarithm properties to obtain:\n",
    "\\begin{align*}\n",
    "\\ell(\\theta) & = \\log \\left[ \\theta^{\\sum_{i=1}^n x_i} (1-\\theta)^{\\sum_{i=1}^n (1-x_i)} \\right] \\\\\n",
    "&=\\log \\left[ \\theta^{\\sum_{i=1}^n x_i} \\right] +  \\log \\left[ (1-\\theta)^{\\sum_{i=1}^n (1-x_i)} \\right] \\\\\n",
    "& = \\left(\\sum_{i=1}^n x_i \\right) \\log [\\theta] + \\left(\\sum_{i=1}^n (1-x_i)\\right) \\log [1 - \\theta]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d574878",
   "metadata": {},
   "source": [
    "**<span style=\"color:blue\">MLE STEP 3: Find the $\\theta^\\star$ that optimizes the log-likelihood function. </span>**\n",
    "This function we just derived is a sum of two logarithms (with linear inputs of $\\theta$), hence log-likelihood in this case is a concave function (it is important to remember that the $x_i$'s are fixed, hence the only variable is $\\theta$). Taking the first order conditions would therefore give us the global maximizer. Formally:\n",
    "\\begin{align*}\n",
    "\\dfrac{ \\mathrm{d}\\left[ \\left(\\sum_{i=1}^n x_i \\right) \\log [\\theta] + \\left(\\sum_{i=1}^n (1-x_i)\\right) \\log [1 - \\theta] \\right]}{\\mathrm{d} \\theta} &= \\frac{\\sum_{i=1}^n x_i}{\\theta} - \\frac{\\sum_{i=1}^n (1-x_i)}{1-\\theta} \\\\\n",
    "& = \\frac{(1-\\theta)\\sum_{i=1}^n x_i}{(1-\\theta)\\theta} - \\frac{\\theta \\sum_{i=1}^n (1-x_i)}{(1-\\theta)\\theta}\n",
    "\\end{align*}\n",
    "and as the optimal $\\theta^\\star$ sets this expression to $0$, we shall have \n",
    "\\begin{align*}\n",
    "& (1-\\theta^\\star)\\sum_{i=1}^n x_i = \\theta^\\star \\sum_{i=1}^n (1-x_i)\\\\\n",
    "\\iff & \\sum_{i=1}^n x_i - \\theta^\\star \\sum_{i=1}^n x_i = \\theta^\\star n - \\theta^\\star \\sum_{i=1}^n x_i \\\\\n",
    "\\iff & \\sum_{i=1}^n x_i = \\theta^\\star n  \\\\\n",
    "\\iff & \\theta^\\star = \\frac{\\sum_{i=1}^n x_i }{n}.\n",
    "\\end{align*}\n",
    "We just deried $\\theta^\\star$. Notice that this is identical to $\\hat{\\theta}$, the unbiased estimator we previously derived. So, the naive estimator that we have been using turns out to be the maximum likelihood estimator. \n",
    "\n",
    "Note: due to the arithmetic operations we implicitly assumed $\\theta$ is not equal to $0$ or $1$. If true $\\theta$ was equal to $0$ or $1$, then all of $x_i$ we observe would be equal to each other, and in this case $\\theta^\\star = \\hat{\\theta} = \\theta$ trivially follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d701d06c",
   "metadata": {},
   "source": [
    "**Exercise Question:** Can you apply Bootstrapping to construct a confidence interval on $\\theta$ when you are only given the realization of $\\mathcal{X}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604a1abc",
   "metadata": {},
   "source": [
    "### MLE in the case of a continuous variable\n",
    "Recall that previously we defined the likelihood function as $\\mathbb{P}[X_1 =x_1, \\ldots, X_n = x_n]$ and since we were working with independent variables this was equal to $\\prod_{i=1}^n \\mathbb{P}[X_i = x_i]$. What if we have a continuous variable? For example, if $X_1$ is normally distributed, what is $\\mathbb{P}[X_1 = x_1]$? The answer is $0$, since a continuous variable on a non-empty support can take uncountably many values, so intuitively there is no chance that we will see a specific value. So the previous application of MLE is not applicable here as the probability of seeing any sample will be $0$ for a continuous variable. \n",
    "\n",
    "To fix this issue, we define the 'likelihood' slighlty different than the 'probability'. Recall that although the probability of $\\mathbb{P}[X_1 = x_1]$ is $0$, we have that $\\mathbb{P}[X_1 \\in [x_1 - \\epsilon, x_1 + \\epsilon]] > 0$ in most cases for small $\\epsilon$. The intuition behind is that although the event $X_1 = x_1$ is impossible to have, there is a positie probability that $X_1$ will be in a (small) neighborhood of $x_1$. We therefore define the likelihood of the event $X_1 = x_1$ via the probability density of $x_1$ assigned by the distribution of $X_1$.\n",
    "\n",
    "**Exercise Question** Let $X$ be a continuous random variable with density function $f(\\cdot)$. What is the relation between $\\mathbb{P}[X \\in [x - \\epsilon, x + \\epsilon]]$ for small $\\epsilon$, and the denisty $f(x)$? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256d323c",
   "metadata": {},
   "source": [
    "Hence, for continuous variables, the likelihood of seeing $\\mathcal{X} = \\{X_1 = x_1, \\ X_2 = x_2, \\ldots, \\ X_n = x_n\\}$ (where $X_i$ are i.i.d. continuous variables) is\n",
    "$$L( \\mathcal{X} ) = \\prod_{i=1}^n f(x_i).$$\n",
    "In general, the likelihood is a function of the parameters that define the density function $f$, and we also represent them in the likelihood notation. For example, if $f$ is parametrized by some $\\alpha, \\ \\beta$, we denote:\n",
    "$$L( \\mathcal{X} | \\alpha, \\beta) = \\prod_{i=1}^n f(x_i).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebe1eae",
   "metadata": {},
   "source": [
    "### MLE of Normal Distributions\n",
    "Let us similarly define $\\mathcal{X} = \\{X_1 = x_1, \\ X_2 = x_2, \\ldots, \\ X_n = x_n \\}$ as the sample we have. This time, $X_i$ are i.i.d. normal variables with mean $\\mu$ and standard deviation $\\sigma$. Let us simulate this for $\\mu = 1$ and $\\sigma = 0.5$ (but recall that in MLE we do not know these values, we just see $\\mathcal{X}$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe545e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "sigma = 0.5 #std\n",
    "mu = 1 #mean\n",
    "n = 200 #let us hage n = 200\n",
    "sample = np.random.normal(mu, sigma, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0d7f51",
   "metadata": {},
   "source": [
    "Recall that the unbiased estimation of $\\mu$ and $\\sigma$ are given as\n",
    "\\begin{align*}\n",
    "& \\hat{\\mu} = \\dfrac{1}{n} \\sum_{i=1}^n x_i \\\\\n",
    "& \\hat{\\sigma} = \\sqrt{\\dfrac{1}{n-1} \\sum_{i=1}^n (x_i - \\hat{\\mu})^2}\n",
    "\\end{align*}\n",
    "Let us next derive the maximum likelihood estimations of $\\mu$ and $\\sigma$ next by using the previous steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4131d6",
   "metadata": {},
   "source": [
    "**<span style=\"color:blue\">MLE STEP 1: Derive the likelihood function. </span>**\n",
    "Since $f(x) = \\dfrac{1}{\\sigma \\sqrt{2\\pi}} \\exp \\left[ - \\dfrac{(x- \\mu)^2}{2 \\sigma^2} \\right]$ is the density function of a normal variable with mean $\\mu$ and standard deviation $\\sigma$, we derive:\n",
    "$$L( \\mathcal{X} | \\mu, \\sigma) = \\prod_{i=1}^n f(x_i) = \\dfrac{1}{(\\sigma \\sqrt{2\\pi})^n} \\prod_{i=1}^n \\exp \\left[ - \\dfrac{(x_i - \\mu)^2}{2 \\sigma^2} \\right] = \\dfrac{1}{(\\sigma \\sqrt{2\\pi})^n} \\exp \\left[ - \\dfrac{\\sum_{i=1}^n (x_i - \\mu)^2}{2 \\sigma^2} \\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360c8bad",
   "metadata": {},
   "source": [
    "**<span style=\"color:blue\">MLE STEP 2: Derive the log-likelihood function. </span>**\n",
    "It immediately follows that:\n",
    "\\begin{align*}\n",
    "\\ell(x | \\mu, \\sigma) &= \\log \\left(\\dfrac{1}{(\\sigma \\sqrt{2\\pi})^n} \\exp \\left[ - \\dfrac{\\sum_{i=1}^n (x_i - \\mu)^2}{2 \\sigma^2} \\right] \\right) \\\\\n",
    "& = - n \\log(\\sigma \\sqrt{2 \\pi}) - \\dfrac{\\sum_{i=1}^n (x_i - \\mu)^2}{2 \\sigma^2}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b212ea",
   "metadata": {},
   "source": [
    "**<span style=\"color:blue\">MLE STEP 3: Find the $\\mu^\\star, \\ \\sigma^\\star$ values that maximize the log-likelihood function. </span>**\n",
    "As before, the log-likelihood function is concave in $\\mu$ and $\\sigma$, hence taking the first order conditions would give us the solution that maximizes the log-likelihood. To this end, we derive:\n",
    "\\begin{align*}\n",
    "& \\dfrac{\\partial \\ell (x | \\mu, \\sigma)}{\\partial \\mu} = \\dfrac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu) \\\\\n",
    "& \\dfrac{\\partial \\ell (x | \\mu, \\sigma)}{\\partial \\sigma} = - \\dfrac{n}{\\sigma} + \\dfrac{1}{\\sigma^3} \\sum_{i=1}^n (x_i - \\mu)^2.\n",
    "\\end{align*}\n",
    "Setting both expressions to $0$ give us the MLE solutions $\\mu^\\star, \\ \\sigma^\\star$ as:\n",
    "\\begin{align*}\n",
    "& \\mu^\\star = \\dfrac{1}{n} \\sum_{i=1}^n x_i\\\\\n",
    "& \\sigma^\\star = \\sqrt{\\dfrac{1}{n} \\sum_{i=1}^n (x_i - {\\mu^\\star})^2}.\n",
    "\\end{align*}\n",
    "Notice that $\\mu^\\star$ is the same as the unbiased estimate $\\hat{\\mu}$. However, $\\sigma^\\star$ is different than the unbiased estimate $\\hat{\\sigma}$ and it is a biased estimation. \n",
    "\n",
    "Next we code to find the MLE solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9594c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE estimator of the mean is 1.035 and the std is 0.511\n"
     ]
    }
   ],
   "source": [
    "mu_star = np.mean(sample)\n",
    "sigma_star = np.sqrt((1/n) * np.sum((sample - mu_star)**2))\n",
    "print(\"MLE estimator of the mean is\", round(mu_star,3), \"and the std is\", round(sigma_star,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81270d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unbiased esimator of the mean is 1.035 and the std is 0.512\n"
     ]
    }
   ],
   "source": [
    "sigma_hat = np.sqrt((1/(n-1)) * np.sum((sample - mu_star)**2))\n",
    "print(\"Unbiased esimator of the mean is\", round(mu_star,3), \"and the std is\", round(sigma_hat,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68401e0e",
   "metadata": {},
   "source": [
    "#### Comparison of MLE and Unbiased estimation of the standard deviation\n",
    "Let us repeat this experiment $10,000$ times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbe67123",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "mu = 1 #mean\n",
    "n = 200 #let us hage n = 200\n",
    "simulation = 10000\n",
    "sigma_star_collection = np.zeros(simulation)\n",
    "sigma_hat_collection = np.zeros(simulation)\n",
    "sigma = 0.5 #std\n",
    "for sim in range(simulation):\n",
    "    sample = np.random.normal(mu, sigma, n)\n",
    "    mu_star = np.mean(sample)\n",
    "    sigma_star_collection[sim] = np.sqrt((1/n) * np.sum((sample - mu_star)**2))\n",
    "    sigma_hat_collection[sim] = np.sqrt((1/(n-1)) * np.sum((sample - mu_star)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a509f197",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As expected, the unbiased estimation of std 0.499 is closer to the true value 0.5 than the MLE estimation 0.498\n"
     ]
    }
   ],
   "source": [
    "print(\"As expected, the unbiased estimation of std\", round(np.mean(sigma_hat_collection),3), \"is closer to the true value\", sigma, \"than the MLE estimation\", round(np.mean(sigma_star_collection),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250e3b40",
   "metadata": {},
   "source": [
    "**Exercise Question** Why does the maximum likelihood estimation of the standard deviation is biased? Why do we ever want to take an estimation which does not converge to the true value of $\\sigma$ if we repeat this experiment many times?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b84d89",
   "metadata": {},
   "source": [
    "### MLE when a closed-form solution is not available: Gamma Distribution\n",
    "The Gamma distribution is a continuous distribution with density:\n",
    "$$f(x | \\alpha, \\beta) = \\dfrac{\\beta^\\alpha x^{\\alpha - 1} e^{- \\beta x}}{\\Gamma(\\alpha)} $$\n",
    "for $x, \\alpha, \\beta > 0$. Here $\\Gamma (\\cdot) $ is [the gamma function](https://en.wikipedia.org/wiki/Gamma_function). Although this distribution is used quite a lot in practice, here we will concentrate on the MLE application only as Step 3 of the MLE scheme will be slightly different than the previous cases. We first sample $n = 200$ i.i.d. Gamma variables with $\\alpha = 1, \\ \\beta = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "342b5328",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "alpha, beta = 1,2\n",
    "n =200\n",
    "sample = np.random.gamma(alpha, (1/beta), n) #instead of beta we give 1/beta as an input due to our definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf112d7",
   "metadata": {},
   "source": [
    "Mean of this distribution is $\\alpha / \\beta = 1 / 2 = 0.5$, hence let us check the sample mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15772063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.493"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(np.mean(sample),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a004ff",
   "metadata": {},
   "source": [
    "Remember that the Gamma function is $\\Gamma(\\alpha) = (\\alpha - 1)!$ for all positive integers $\\alpha$. In general for $\\alpha > 0$ the formal definition of this function is:\n",
    "$$\\Gamma(\\alpha) := \\int_{0}^{\\infty} t^{\\alpha - 1} e^{-t} \\mathrm{d}t.$$\n",
    "Python's Math module has a built-in Gamma function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1037967b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.gamma(5) == math.factorial(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02f1e9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.gamma(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8e8f361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.93175373836837"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.gamma(5.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1813fbe",
   "metadata": {},
   "source": [
    "We are now ready do apply MLE to estimate $\\alpha$ and $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bffda2",
   "metadata": {},
   "source": [
    "**<span style=\"color:blue\">MLE STEP 1: Derive the likelihood function. </span>**\n",
    "$$L(\\mathcal{X} | \\alpha, \\beta) = \\prod_{i=1}^n f(x_i) = \\prod_{i=1}^n \\dfrac{\\beta^\\alpha x_i^{\\alpha - 1} e^{- \\beta x_i}}{\\Gamma(\\alpha)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca03d55",
   "metadata": {},
   "source": [
    "**<span style=\"color:blue\">MLE STEP 2: Derive the log-likelihood function. </span>**\n",
    "\\begin{align*}\n",
    "\\ell(\\mathcal{X} | \\alpha, \\beta) &= \\log \\left[ \\prod_{i=1}^n \\dfrac{\\beta^\\alpha x_i^{\\alpha - 1} e^{- \\beta x_i}}{\\Gamma(\\alpha)} \\right] \\\\\n",
    "& = \\sum_{i=1}^n  \\left[ \\log \\left(\\beta^\\alpha x_i^{\\alpha - 1} e^{- \\beta x_i}\\right)  - \\log\\left(\\Gamma(\\alpha)\\right)\\right] \\\\\n",
    "& = \\sum_{i=1}^n  \\left[ \\alpha \\log (\\beta) + (\\alpha - 1) \\log (x_i) - \\beta x_i - \\log\\left(\\Gamma(\\alpha)\\right)\\right] \\\\\n",
    "& = n \\alpha \\log(\\beta) - n \\log(\\Gamma(\\alpha)) + (\\alpha - 1) \\sum_{i=1}^n \\log (x_i) - \\beta \\sum_{i=1}^n x_i.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea0c8f8",
   "metadata": {},
   "source": [
    "**<span style=\"color:blue\">MLE STEP 3: Find the $\\mu^\\star, \\ \\sigma^\\star$ values that maximize the log-likelihood function. </span>**\n",
    "Similarly as before, we can take the first order conditions to solve maximization of $\\ell(\\mathcal{X} | \\alpha,\\beta)$. Let us start by doing this for $\\beta$:\n",
    "\\begin{align*}\n",
    "\\dfrac{\\partial \\ell(\\mathcal{X} | \\alpha, \\beta)}{\\partial \\beta} = \\dfrac{n\\alpha}{\\beta} - \\sum_{i=1}^n x_i,\n",
    "\\end{align*}\n",
    "hence, by setting this expression to $0$, the MLE estimator of $\\beta$ can be obtained as:\n",
    "\\begin{align*}\n",
    "\\beta^\\star = \\dfrac{n \\alpha}{\\sum_{i=1}^n x_i}.\n",
    "\\end{align*}\n",
    "We substitute this in the log-likelihood function to obtain:\n",
    "\\begin{align*}\n",
    "\\ell\\left(\\mathcal{X} | \\alpha, \\dfrac{n \\alpha}{\\sum_{i=1}^n x_i} \\right) = n\\alpha \\log\\left( \\dfrac{n \\alpha}{\\sum_{i=1}^n x_i} \\right) - n \\log(\\Gamma(\\alpha)) + (\\alpha - 1) \\sum_{i=1}^n \\log(x_i)   - n\\alpha.\n",
    "\\end{align*}\n",
    "However, unfortunately, the first-order conditions of the log-likelihood function does not admit a closed form solution for $\\alpha$. We therefore need to use an optimization algorithm to numerically optimize the $\\alpha$ value.\n",
    "\n",
    "Note (detail): $\\Gamma$ is a logarithmically convex function, meaning theat $\\log(\\Gamma(\\cdot))$ is a convex function, hence $- \\log(\\Gamma(\\cdot))$ is concave.\n",
    "\n",
    "Let us code this process. For simplicity, we assume that $\\alpha$ lies in the region $[0.1, 10]$. We first define the log-likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf768e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle_gamma(alpha, n, sample):\n",
    "    '''\n",
    "    Objective function - log-likelihood of the gamma distribution wrt the parameter 'alpha'\n",
    "    '''\n",
    "    return n*alpha*math.log((n * alpha)/np.sum(sample)) - n*math.log(math.gamma(alpha)) + (alpha * np.sum(np.log(sample))) - n*alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c218ab8",
   "metadata": {},
   "source": [
    "Next, we plot the log-likelihood function on the domain of $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67766661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let us plot the log-likelihood function\n",
    "# linearly spaced alphas\n",
    "alphas = np.linspace(0.1,10,1000)\n",
    "# the function as we just defined above\n",
    "y = [mle_gamma(alpha, n, sample) for alpha in alphas]\n",
    "\n",
    "fig = plt.figure()\n",
    "# below taken from https://scriptverse.academy/tutorials/python-matplotlib-plot-function.html\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.spines['left'].set_position('center')\n",
    "ax.spines['bottom'].set_position('center')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "\n",
    "# plot the function\n",
    "plt.axvline(x =1, color='r', linestyle='--', label = r\"true value of $\\alpha$\")\n",
    "\n",
    "plt.plot(alphas ,y, 'g', label = r\"log-likelihood\")\n",
    "plt.legend()\n",
    "plt.title(\"The horizontal axis is the alpha values and vertical axis is the corresponding log-likelihood\")\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c195b782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0315315315315317"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#which (discretized!) alpha givse the maximal log-likelihood above?\n",
    "alphas[np.argmax(y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d49301",
   "metadata": {},
   "source": [
    "**Exercise Question** Do you see how the log-likelihood is shaped concave? Show that this is not coincidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6a898b",
   "metadata": {},
   "source": [
    "As we know that the log-likelihood is concave, and as $\\alpha \\in [0.1, 10]$, we can search the optimal value by using a [bisection search](https://en.wikipedia.org/wiki/Bisection_method). We derive this next. First of all, we should derive the derivative of the log-likelihood function and set it equal to zero (even though it doesn't admit a closed form solution, we will implement a search algorithm). Denote $\\psi$ as the [digamma function](https://en.wikipedia.org/wiki/Digamma_function). Hence we need:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebe3cb1",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\log(\\alpha) - \\psi(\\alpha) - \\log\\left( \\dfrac{1}{n} \\sum_{i=1}^n x_i \\right) + \\dfrac{1}{n}\\sum_{i=1}^n \\log(x_i) = 0.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092df162",
   "metadata": {},
   "source": [
    "Let us implement this derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "384b6b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_mle_gamma(alpha, n, sample):\n",
    "    '''\n",
    "    Objective function - log-likelihood of the gamma distribution wrt the parameter 'alpha'\n",
    "    '''\n",
    "    return math.log(alpha) - psi(alpha) - math.log((1/n)* np.sum(sample)) + (1/n) * np.sum(np.log(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f877d",
   "metadata": {},
   "source": [
    "What we observed in the plot above is that, when the log-likelihood function is concave, before it is at the global optimum value its slope is positive (hence a positive derivative), then at its global optimum value the slope is $0$, and afterwards the slope gets negative. Hence, we can apply the bisection search as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe6b84d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "right = 10\n",
    "left = 0.1\n",
    "while(abs(right - left) >= 0.00001):\n",
    "    center = (right + left) / 2\n",
    "    if derivative_mle_gamma(center, n, sample) > 0:\n",
    "        left = center\n",
    "    else:\n",
    "        right = center\n",
    "alpha_star = center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f580f84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE estimation of alpha is 1.033\n"
     ]
    }
   ],
   "source": [
    "print(\"MLE estimation of alpha is\", np.round(alpha_star,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9f2318",
   "metadata": {},
   "source": [
    "Recall also that we derived\n",
    "\\begin{align*}\n",
    "\\beta^\\star = \\dfrac{n \\alpha}{\\sum_{i=1}^n x_i},\n",
    "\\end{align*}\n",
    "hence by using $\\alpha^\\star$ as $\\alpha$ we can also obtain $\\beta^\\star$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0daccc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE estimation of beta is 2.097\n"
     ]
    }
   ],
   "source": [
    "beta_star = n*alpha_star / (np.sum(sample))\n",
    "print(\"MLE estimation of beta is\", np.round(beta_star,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbc6e0b",
   "metadata": {},
   "source": [
    "Not bad at alll, given that true values are $\\alpha = 1, \\ \\beta = 2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc7e1a0",
   "metadata": {},
   "source": [
    "#### Let's see if the estimation is unbiased empirically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "412ae5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) #random seed\n",
    "alpha = 1 #true parameter alpha\n",
    "beta = 2 #true parameter beta\n",
    "n = 2000  #number of observations in a sample\n",
    "simulation = 100 #how many times to simulate this experiment\n",
    "alpha_star_collection = np.zeros(simulation) #vector of MLE estimates of alpha in each simulation\n",
    "beta_star_collection = np.zeros(simulation)  #vector of MLE estimates of beta in each simulation\n",
    "for sim in range(simulation): #simulate\n",
    "    sample = np.random.gamma(alpha, (1/beta), n) #sample n iid Gamma variables\n",
    "    right = 10 #bisection right-hand side\n",
    "    left = 0.1 #bisection lhs\n",
    "    while(abs(right - left) >= 0.00001): #until 'convergence'\n",
    "        #try to understand the body of this while-loop! Does it also work like this if log-likelihood was not concave?\n",
    "        center = (right + left) / 2 \n",
    "        if derivative_mle_gamma(center, n, sample) > 0:\n",
    "            left = center\n",
    "        else:\n",
    "            right = center\n",
    "    alpha_star = center\n",
    "    beta_star = n*alpha_star / (np.sum(sample))\n",
    "    alpha_star_collection[sim] = alpha_star\n",
    "    beta_star_collection[sim] = beta_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d0354508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.002"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(np.mean(alpha_star_collection),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "089ec35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.999"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(np.mean(beta_star_collection),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bbbafe",
   "metadata": {},
   "source": [
    "Overall, you will see that as $n$ increases, the estimations will converge to the true values, which is almost always the case. The reason is that we are optimizing the likelihood of the sample, and as $n$ increases, the sample will converge to the population in the sense of 'generelization'. \n",
    "\n",
    "However, for a fixed $n$ (e.g., $200$), increasing ```simulation``` does not converge these values to the true numbers, concluding that such a method is not necessarily unbiased. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
